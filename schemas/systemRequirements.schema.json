{
    "$schema": "https://json-schema.org/draft/2020-12/schema",
    "title": "System Requirements Entry Point Mapping",
    "type": "object",
    "description": "System requirements mapping for entry points. Keys are entry point names (e.g., 'main', 'process') or '*' for all entry points.",
    "markdownDescription": "**System Requirements**  \nDefine system requirements for each entry point. Keys are entry point names (e.g., `\"main\"`, `\"process\"`), or use `\"*\"` to apply requirements to all entry points.\n\n**Example:**\n```json\n{\n  \"*\": {\n    \"instanceType\": \"mem1_ssd1_v2_x4\"\n  },\n  \"main\": {\n    \"instanceType\": \"mem1_ssd1_v2_x8\"\n  }\n}\n```",
    "additionalProperties": {
        "type": "object",
        "description": "Requirements for a specific entry point.",
        "markdownDescription": "**Entry Point Requirements**  \nSystem requirements for this entry point.",
        "properties": {
            "instanceType": {
                "type": "string",
                "description": "Instance type for this entry point. Must match the cloud provider for the region.",
                "markdownDescription": "**Instance Type**  \nInstance type for this entry point. The value must be appropriate for the cloud provider of the region where the app runs."
            },
            "fpgaDriver": {
                "type": "string",
                "description": "FPGA driver to install on FPGA-enabled instances. Accepted values: 'edico-1.4.9.2' (for mem3_ssd2_fpga1_x24, mem3_ssd2_fpga2_x48, mem3_ssd2_fpga8_x192), 'edico-1.4.2', 'edico-1.4.5', 'edico-1.4.7' (for mem3_ssd2_fpga1_x8, mem3_ssd2_fpga1_x16, mem3_ssd2_fpga1_x64).",
                "markdownDescription": "**FPGA Driver** (optional)  \nSpecifies the FPGA driver to install on the FPGA-enabled cloud host instance before app code execution.\n\n**Accepted values by instance type:**\n- `mem3_ssd2_fpga1_x24`, `mem3_ssd2_fpga2_x48`, `mem3_ssd2_fpga8_x192`: `edico-1.4.9.2` (default)\n- `mem3_ssd2_fpga1_x8`, `mem3_ssd2_fpga1_x16`, `mem3_ssd2_fpga1_x64`: `edico-1.4.2` (default), `edico-1.4.5`, `edico-1.4.7`"
            },
            "nvidiaDriver": {
                "type": "string",
                "description": "NVIDIA driver to install on GPU-enabled instances. Accepted values: 'R470' (default, driver 470.256.02, CUDA 11.4), 'R535' (driver 535.247.01, CUDA 12.2).",
                "markdownDescription": "**NVIDIA Driver** (optional)  \nSpecifies the NVIDIA driver to install on the GPU-enabled cloud host instance before app code execution.\n\n**Accepted values:**\n- `R470` (default): Driver version [470.256.02](https://docs.nvidia.com/datacenter/tesla/tesla-release-notes-470-256-02/index.html), supports CUDA 11.4\n- `R535`: Driver version [535.247.01](https://docs.nvidia.com/datacenter/tesla/tesla-release-notes-535-247-01/index.html), supports CUDA 12.2"
            },
            "clusterSpec": {
                "type": "object",
                "description": "Cluster configuration for distributed computing.",
                "markdownDescription": "**Cluster Specification**  \nConfigure a Spark cluster for distributed computing workloads. When the app is launched, the system instantiates a Spark cluster specifically for that execution.\n\n**Cluster Management:**  \nSpark apps use [Spark Standalone](https://spark.apache.org/docs/latest/spark-standalone.html) for cluster management and [HDFS](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html) to share data between nodes.\n\n**Cluster Lifecycle:**  \nThe job lifecycle reflects the state of the driver node. A Spark job fails if the driver node fails. If a clusterWorker node fails, a new node is automatically provisioned.",
                "properties": {
                    "type": {
                        "type": "string",
                        "enum": [
                            "spark",
                            "dxspark",
                            "apachespark"
                        ],
                        "markdownEnumDescriptions": [
                            "Alias for `dxspark` - Apollo Spark cluster (fully integrated with DNAnexus Platform, includes Hive Metastore integration, database objects, and SQL permissions)",
                            "Apollo Spark cluster - Fully integrated with DNAnexus Platform. Database objects are created in the same project. Requires network access and VIEW/UPLOAD project permissions.",
                            "Generic Apache Spark cluster - Open-source Spark without Platform integration. Use for custom storage locations (e.g., S3). No database objects created by Platform."
                        ],
                        "description": "Type of cluster. Supported values (case-insensitive): 'spark' (alias for dxspark), 'dxspark', 'apachespark'.",
                        "markdownDescription": "**Cluster Type**  \nThe type of cluster to provision. Supported values (case-insensitive):  \n- `spark` or `dxspark` - Apollo Spark cluster (fully integrated with DNAnexus Platform, includes Hive Metastore integration, database objects created in project, requires network access and VIEW/UPLOAD permissions)  \n- `apachespark` - Generic Apache Spark cluster (open-source Spark without Platform integration, use for custom storage locations like S3)"
                    },
                    "version": {
                        "type": "string",
                        "enum": [
                            "2.4.4",
                            "3.2.0",
                            "3.2.3",
                            "3.5.2"
                        ],
                        "markdownEnumDescriptions": [
                            "Spark 2.4.4 - Compatible with Ubuntu 14.04/16.04 (Python 2.7.x or 3.5.x)",
                            "Spark 3.2.0 - Compatible with Ubuntu 20.04 (Python 3.8.10)",
                            "Spark 3.2.3 - Compatible with Ubuntu 20.04 (Python 3.8.10)",
                            "Spark 3.5.2 - Compatible with Ubuntu 20.04 (Python 3.8.10)"
                        ],
                        "description": "Software version for the cluster. Required. Supported: '2.4.4', '3.2.0', '3.2.3', '3.5.2'.",
                        "markdownDescription": "**Version**  \nApache Spark version for the cluster. Must be compatible with the runSpec distribution/release.\n\n**Compatibility:**  \n- `2.4.4`: Ubuntu 14.04/16.04  \n- `3.2.0`, `3.2.3`, `3.5.2`: Ubuntu 20.04"
                    },
                    "initialInstanceCount": {
                        "type": "integer",
                        "minimum": 1,
                        "description": "Number of nodes in the cluster, including the driver. Min value 1 indicates a single-node cluster where both driver and clusterWorker run on the same node.",
                        "markdownDescription": "**Initial Instance Count**  \nThe total number of nodes in the cluster, including the driver node. Minimum value is 1.\n\n**Configuration:**  \n- Value of `1`: Single-node Spark cluster where both driver and clusterWorker run on the same node  \n- Value > `1`: Multi-node cluster with one driver node and (n-1) clusterWorker nodes\n\n**Node Details:**  \n- **Driver node**: Runs Spark master service, driver, HDFS namenode/datanode, and application code  \n- **ClusterWorker nodes**: Run Spark clusterWorker service and executors that process data\n\n**Note:** This is the only `clusterSpec` field that can be overridden at runtime using the `--instance-count` flag with `dx run`. All nodes have identical hardware and software configurations (same instance type, packages, and resources)."
                    },
                    "ports": {
                        "type": "string",
                        "description": "Comma-delimited string of additional ports/ranges to open between cluster nodes (e.g., '9500, 9700-9750'). Default ports are included automatically for spark/dxspark/apachespark.",
                        "markdownDescription": "**Ports** (optional)  \nComma-delimited string specifying additional ports or port ranges to open between cluster nodes for inter-node communication.\n\n**Example:** `\"9500, 9700-9750\"` opens port 9500 and ports 9700-9750\n\n**Important:**  \n- The system automatically includes default ports required by Spark for `dxspark` and `apachespark` cluster types  \n- Only specify additional custom ports your application needs  \n- Some ports are reserved for Platform internal functions and should not be used  \n- Empty string or omit to use defaults only"
                    },
                    "bootstrapScript": {
                        "type": "string",
                        "description": "Path to the bootstrap script run on all cluster nodes before the driver begins running the application code.",
                        "markdownDescription": "**Bootstrap Script** (optional)  \nPath to a bootstrap script (relative to the app/applet source directory) that runs on **all cluster nodes** (both driver and clusterWorker nodes) before the driver begins running the application code.\n\n**Use Cases:**  \n- Installing additional software or services  \n- Downloading data needed on all nodes  \n- Environment configuration\n\n**Execution:**  \nThe script runs after packages are installed but before Spark services start. Use environment variable `$DX_CLUSTER_MASTER_IP` to determine if running on driver (empty) or worker (set) node.\n\n**Note:** In dxapp.json, specify the filename/path only (e.g., `\"src/bootstrap.sh\"`). The file content will be embedded by `dx build`. It's recommended to locate the script in the same location as application code."
                    },
                    "slaveInstanceType": {
                        "type": "string",
                        "description": "Instance type for clusterWorker (worker) nodes. If not specified, workers use the same instance type as the driver.",
                        "markdownDescription": "**Slave Instance Type** (optional)  \nThe instance type for clusterWorker (worker) nodes (e.g., `mem1_ssd1_v2_x4`).\n\n**Default behavior:** If not specified, worker nodes use the same instance type as the driver node (specified in `instanceType`).\n\n**Note:** This property uses legacy terminology (`slave`). In current DNAnexus documentation, worker nodes are referred to as `clusterWorker` nodes."
                    }
                },
                "required": [
                    "type",
                    "version",
                    "initialInstanceCount"
                ],
                "additionalProperties": false
            }
        },
        "additionalProperties": false
    }
}